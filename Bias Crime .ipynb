{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0bb72e-505c-42f3-aad5-6069cb771bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ruptures as rpt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Dropout, Flatten, Dense, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Flatten, Conv1D, BatchNormalization,\n",
    "    Activation, Dropout, Add, Dense, Concatenate, GRU\n",
    ")\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e71c0-e969-4a88-90b6-84554ad9f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_crime_path = \"hate_crime.csv\"\n",
    "data_crime = pd.read_csv(hate_crime_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cbffa-0656-4772-9dbf-18cc4bbe3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crime.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f05a8c-e6ff-49d5-9cca-0ef5d9651017",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crime.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433c7c27-97e8-46a1-985d-7ca78ffcf4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crime.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8283c-8628-4cbe-b492-486a895beca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crime.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea94695-52eb-43ee-99a0-79f92c43bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Converting incident_date to datetime\n",
    "data_crime['incident_date'] = pd.to_datetime(data_crime['incident_date'], errors='coerce')\n",
    "\n",
    "# Adding year column for trend analysis\n",
    "data_crime['year'] = data_crime['incident_date'].dt.year\n",
    "\n",
    "# Top 10 hate crime bias types\n",
    "plt.figure(figsize=(10,5))\n",
    "data_crime['bias_desc'].value_counts().head(10).plot(kind='barh')\n",
    "plt.title('Top 10 Hate Crime Bias Types')\n",
    "plt.xlabel('Number of Incidents')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303becf-bc9d-4845-afb3-b72c09fe0745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 offense types\n",
    "plt.figure(figsize=(10,9))\n",
    "data_crime['offense_name'].value_counts().head(10).plot(kind='bar')\n",
    "plt.title('Top 10 Offense Types')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed4b88-0f8b-41ad-bc2f-0e9472ae6de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 states by number of incidents\n",
    "plt.figure(figsize=(12,6))\n",
    "data_crime['state_name'].value_counts().head(15).plot(kind='bar')\n",
    "plt.title('Top 15 States by Hate Crime Count')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a7233-3625-4bec-b003-6fe02a59377a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27ea88-f0a1-4239-baaf-5580b194ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initial Cleaning of data\n",
    "data_crime.drop(\"pub_agency_unit\", axis=1, inplace=True)\n",
    "data_crime['incident_date'] = pd.to_datetime(data_crime['incident_date'], errors='coerce')\n",
    "data_crime['year'] = data_crime['incident_date'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e240b0-23ea-4cd4-86aa-37fdf3a9d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. missing numerical fields\n",
    "fill_zero_cols = [\n",
    "    'adult_victim_count', 'juvenile_victim_count', 'adult_offender_count',\n",
    "    'juvenile_offender_count', 'total_individual_victims'\n",
    "]\n",
    "data_crime[fill_zero_cols] = data_crime[fill_zero_cols].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179b929-84b4-4b05-97d8-1a10fcbfcc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Categorize Bias\n",
    "def categorize_bias(bias):\n",
    "    if isinstance(bias, str):\n",
    "        if 'Race' in bias or 'Black' in bias or 'White' in bias or 'Asian' in bias:\n",
    "            return 'Race'\n",
    "        elif 'Jewish' in bias or 'Islamic' in bias or 'Christian' in bias or 'Religion' in bias:\n",
    "            return 'Religion'\n",
    "        elif 'Gender' in bias or 'Sexual' in bias or 'Transgender' in bias:\n",
    "            return 'Gender/Sexuality'\n",
    "        elif 'Disability' in bias:\n",
    "            return 'Disability'\n",
    "        elif 'Ethnicity' in bias or 'Hispanic' in bias:\n",
    "            return 'Ethnicity'\n",
    "    return 'Other'\n",
    "\n",
    "data_crime['bias_category'] = data_crime['bias_desc'].apply(categorize_bias)\n",
    "\n",
    "# 6. Group by Year and Bias\n",
    "yearly_bias = data_crime.groupby(['year', 'bias_category']).agg({\n",
    "    'victim_count': 'sum',\n",
    "    'total_offender_count': 'sum',\n",
    "    'offense_name': 'count'\n",
    "}).rename(columns={'offense_name': 'incident_count'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51825217-1f61-4f54-9230-01c289dcddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Feature Engineering\n",
    "yearly_bias['incident_rolling'] = yearly_bias.groupby('bias_category')['incident_count'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "yearly_bias['yoy_change'] = yearly_bias.groupby('bias_category')['incident_count'].pct_change()\n",
    "yearly_bias['incident_lag1'] = yearly_bias.groupby('bias_category')['incident_count'].shift(1)\n",
    "yearly_bias['incident_lag2'] = yearly_bias.groupby('bias_category')['incident_count'].shift(2)\n",
    "model_data = yearly_bias.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b99767-a861-402b-9ea6-afa2dfa67380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. One-Hot Encode Bias Category\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "bias_encoded = encoder.fit_transform(model_data[['bias_category']])\n",
    "bias_encoded_df = pd.DataFrame(bias_encoded, columns=encoder.get_feature_names_out(['bias_category']), index=model_data.index)\n",
    "\n",
    "# 9. Build Feature Matrix\n",
    "X = pd.concat([\n",
    "    model_data[['year', 'incident_rolling', 'yoy_change', 'incident_lag1', 'incident_lag2']],\n",
    "    bias_encoded_df\n",
    "], axis=1)\n",
    "y = model_data['incident_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70908ea4-2735-4b46-a8c0-d738f72b3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 11. Train & Tune Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=rf_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf_best = best_rf.predict(X_test)\n",
    "rmse_best_rf = mean_squared_error(y_test, y_pred_rf_best, squared=False)\n",
    "mae_best_rf = mean_absolute_error(y_test, y_pred_rf_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b41c899-d91c-4d60-8d23-0a72c7febf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Best Params:\", grid_search_rf.best_params_)\n",
    "print(f\"Random Forest RMSE: {rmse_best_rf:.2f}, MAE: {mae_best_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51514a2a-3fff-49a4-b181-36bfed92fe11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 12. Train & Tune LightGBM\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'min_child_samples': [5, 10]\n",
    "}\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "grid_search_lgb = GridSearchCV(estimator=lgb_model, param_grid=lgb_param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_lgb.fit(X_train, y_train)\n",
    "best_lgb = grid_search_lgb.best_estimator_\n",
    "y_pred_lgb = best_lgb.predict(X_test)\n",
    "rmse_lgb = mean_squared_error(y_test, y_pred_lgb, squared=False)\n",
    "mae_lgb = mean_absolute_error(y_test, y_pred_lgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca38e7-5254-4d16-8897-d3535fb3b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LightGBM Best Params:\", grid_search_lgb.best_params_)\n",
    "print(f\"LightGBM RMSE: {rmse_lgb:.2f}, MAE: {mae_lgb:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c07b8c-b3b6-4c44-85d6-4b31aaa7ccc9",
   "metadata": {},
   "source": [
    "TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc7bc1-68db-42b7-8022-a121686be560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Define columns based on your schema\n",
    "categorical_cols = ['state_abbr', 'offender_race', 'offender_ethnicity', 'offense_name', 'bias_desc']\n",
    "numerical_cols = [\n",
    "    'adult_victim_count', 'juvenile_victim_count', 'total_offender_count',\n",
    "    'adult_offender_count', 'juvenile_offender_count',\n",
    "    'victim_count', 'total_individual_victims'\n",
    "]\n",
    "target_col = 'victim_count' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1179c42-3fe6-42f1-9197-aa31ca1cfa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STEP 3: Encode categorical features and scale numerics\n",
    "data_crime = data_crime.dropna(subset=numerical_cols + categorical_cols + [target_col])  # Drop rows with missing critical data\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    data_crime[col + '_enc'] = le.fit_transform(data_crime[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_crime[numerical_cols] = scaler.fit_transform(data_crime[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542affcb-2bda-4092-b26b-66f085182447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Build sequences\n",
    "WINDOW_SIZE = 5\n",
    "cat_feature_array = data_crime[[col + '_enc' for col in categorical_cols]].values\n",
    "num_feature_array = data_crime[numerical_cols].values\n",
    "y_values = data_crime[target_col].values\n",
    "\n",
    "X_seq_num, X_seq_cat, y_seq = [], [], []\n",
    "for i in range(WINDOW_SIZE, len(data_crime)):\n",
    "    X_seq_num.append(num_feature_array[i - WINDOW_SIZE:i])\n",
    "    X_seq_cat.append(cat_feature_array[i])\n",
    "    y_seq.append(y_values[i])\n",
    "\n",
    "X_seq_num = np.array(X_seq_num)\n",
    "X_seq_cat = np.array(X_seq_cat)\n",
    "y_seq = np.array(y_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5c516-dde6-4b6d-b3e7-47f233e9e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STEP 5: Train/test split\n",
    "train_size = int(len(X_seq_num) * 0.8)\n",
    "X_train_seq = X_seq_num[:train_size]\n",
    "X_test_seq = X_seq_num[train_size:]\n",
    "X_train_cat = X_seq_cat[:train_size]\n",
    "X_test_cat = X_seq_cat[train_size:]\n",
    "y_train = y_seq[:train_size]\n",
    "y_test = y_seq[train_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766a7db-5319-46d0-a56e-0d260927e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Build TCN model with embeddings\n",
    "def build_tcn_model(hp):\n",
    "    seq_input = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]), name=\"seq_input\")\n",
    "\n",
    "    embed_inputs = []\n",
    "    embeddings = []\n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        num_classes = len(label_encoders[col].classes_)\n",
    "        cat_input = Input(shape=(1,), name=f'{col}_input')\n",
    "        embed = Embedding(input_dim=num_classes, output_dim=hp.Int(f'{col}_embed_dim', 4, 16))(cat_input)\n",
    "        embed = Flatten()(embed)\n",
    "        embed_inputs.append(cat_input)\n",
    "        embeddings.append(embed)\n",
    "\n",
    "    # TCN layers\n",
    "    x = seq_input\n",
    "    for i in range(hp.Int('num_blocks', 1, 2)):\n",
    "        res = x\n",
    "        x = Conv1D(\n",
    "            filters=hp.Int(f'filters_{i}', 32, 128, step=32),\n",
    "            kernel_size=hp.Choice(f'kernel_size_{i}', [2, 3, 4]),\n",
    "            padding='causal'\n",
    "        )(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}', 0.1, 0.5, step=0.1))(x)\n",
    "        if res.shape[-1] == x.shape[-1]:\n",
    "            x = Add()([x, res])\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    merged = Concatenate()([x] + embeddings)\n",
    "    x = Dense(hp.Int('dense_units', 32, 128, step=32), activation='relu')(merged)\n",
    "    x = Dropout(hp.Float('dense_dropout', 0.1, 0.5, step=0.1))(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[seq_input] + embed_inputs, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Choice('lr', [1e-2, 1e-3, 5e-4])),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d6819-e464-4bf4-97ea-153d107de7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Hyperparameter tuning\n",
    "tuner = kt.RandomSearch(\n",
    "    build_tcn_model,\n",
    "    objective='val_mae',\n",
    "    max_trials=10,\n",
    "    directory='tcn_tuning_final',\n",
    "    project_name='hatecrime_tcn_embed'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "tuner.search(\n",
    "    x=[X_train_seq] + [X_train_cat[:, i] for i in range(X_train_cat.shape[1])],\n",
    "    y=y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa4ca3-cadb-4936-99d9-ed44df9b0484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Evaluate best model\n",
    "best_tcn_model = tuner.get_best_models(1)[0]\n",
    "preds = best_tcn_model.predict([X_test_seq] + [X_test_cat[:, i] for i in range(X_test_cat.shape[1])]).flatten()\n",
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "print(f\"\\n Tuned TCN RMSE: {rmse:.2f}\")\n",
    "print(f\" Tuned TCN MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519af73-9649-427c-bd82-637bf02fe2d1",
   "metadata": {},
   "source": [
    "GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e4766-ffd8-43d6-9cfa-1218b51a4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "train_size = int(len(X_seq_num) * 0.8)\n",
    "X_train_seq, X_test_seq = X_seq_num[:train_size], X_seq_num[train_size:]\n",
    "X_train_cat, X_test_cat = X_seq_cat[:train_size], X_seq_cat[train_size:]\n",
    "y_train_seq, y_test_seq = y_seq[:train_size], y_seq[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4cbe4-95f3-4ac3-9fcb-b0588f5e95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Build ST-GRU Model with Embeddings\n",
    "def build_stgru_model(hp):\n",
    "    # Numerical sequence input\n",
    "    seq_input = Input(shape=(WINDOW_SIZE, X_train_seq.shape[2]), name=\"seq_input\")\n",
    "\n",
    "    # Categorical embeddings\n",
    "    embed_inputs = []\n",
    "    embeddings = []\n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        vocab_size = len(label_encoders[col].classes_)\n",
    "        input_cat = Input(shape=(1,), name=f\"{col}_input\")\n",
    "        embed_dim = hp.Int(f\"{col}_embed_dim\", 4, 16)\n",
    "        embed = Embedding(input_dim=vocab_size, output_dim=embed_dim)(input_cat)\n",
    "        embed = Flatten()(embed)\n",
    "        embed_inputs.append(input_cat)\n",
    "        embeddings.append(embed)\n",
    "\n",
    "    # GRU output from sequence\n",
    "    gru_units = hp.Int(\"gru_units\", 32, 128, step=32)\n",
    "    x = GRU(gru_units)(seq_input)\n",
    "\n",
    "    # Merge all\n",
    "    x = Concatenate()([x] + embeddings)\n",
    "    dense_units = hp.Int(\"dense_units\", 32, 128, step=32)\n",
    "    x = Dense(dense_units, activation='relu')(x)\n",
    "    x = Dropout(hp.Float(\"dropout\", 0.1, 0.5, step=0.1))(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[seq_input] + embed_inputs, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Choice(\"lr\", [1e-2, 1e-3, 5e-4])),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82376d-b8bd-4292-ab4e-1a6778d1560f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STEP 5: Hyperparameter Tuning with KerasTuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_stgru_model,\n",
    "    objective='val_mae',\n",
    "    max_trials=2,\n",
    "    executions_per_trial=1,\n",
    "    directory='stgru_tuning_final',\n",
    "    project_name='hatecrime_stgru_embed'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(\n",
    "    x=[X_train_seq] + [X_train_cat[:, i] for i in range(X_train_cat.shape[1])],\n",
    "    y=y_train_seq,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c6b80-8e87-4073-91e9-8b27d4b87534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Evaluate the Best Model\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "preds = best_model.predict([X_test_seq] + [X_test_cat[:, i] for i in range(X_test_cat.shape[1])]).flatten()\n",
    "rmse = mean_squared_error(y_test_seq, preds, squared=False)\n",
    "mae = mean_absolute_error(y_test_seq, preds)\n",
    "\n",
    "print(f\"\\n Tuned ST-GRU RMSE: {rmse:.2f}\")\n",
    "print(f\" Tuned ST-GRU MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186bc90-8538-4963-ab7c-631deec2bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create SHAP Explainer\n",
    "explainer = shap.Explainer(best_lgb)\n",
    "\n",
    "# Use the same test set as in model evaluation\n",
    "X_test_df = X_test.copy()\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer(X_test_df)\n",
    "\n",
    "# Summary Plot (Global Feature Importance)\n",
    "shap.plots.beeswarm(shap_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e68179-ae3f-4ec4-9e53-73da3fbf31d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap.plots.bar(shap_values, max_display=10)\n",
    "\n",
    "shap.plots.scatter(shap_values[:, \"incident_lag1\"], color=shap_values)\n",
    "\n",
    "sample_index = 10\n",
    "shap.plots.waterfall(shap_values[sample_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7a7df-1791-4cd2-94af-39a1c1b3842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract city and state\n",
    "data_crime['city'] = data_crime['pug_agency_name'].str.extract(r'([A-Za-z\\s]+)')\n",
    "data_crime['state'] = data_crime['state_name']\n",
    "\n",
    "# Simulated sample US cities dataset (replace with full file for production use)\n",
    "cities_sample =  pd.read_csv(\"uscities.csv\")\n",
    "# Normalize text\n",
    "data_crime['city'] = data_crime['city'].str.lower().str.strip()\n",
    "data_crime['state'] = data_crime['state'].str.lower().str.strip()\n",
    "cities_sample['city'] = cities_sample['city'].str.lower().str.strip()\n",
    "cities_sample['state_name'] = cities_sample['state_name'].str.lower().str.strip()\n",
    "\n",
    "# Merge datasets\n",
    "merged = data_crime.merge(cities_sample, left_on=['city', 'state'], right_on=['city', 'state_name'], how='left')\n",
    "merged = merged.rename(columns={'lat': 'latitude', 'lng': 'longitude'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898049bc-c9fb-46ba-8e1a-e71d2106e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Filter rows with valid coordinates\n",
    "map_data = merged.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Create a folium map centered on the U.S.\n",
    "map_center = [39.8283, -98.5795]  # Geographic center of contiguous USA\n",
    "m = folium.Map(location=map_center, zoom_start=4)\n",
    "\n",
    "# Add markers to the map (sample if too large)\n",
    "for _, row in map_data.sample(n=175000, random_state=42).iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=3,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_opacity=0.6\n",
    "    ).add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38c1cc-2aca-4040-b597-076c0222d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruptures as rpt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define list of bias categories to analyze\n",
    "bias_categories = yearly_bias['bias_category'].unique()\n",
    "\n",
    "# Detect and visualize change points for each category\n",
    "for category in bias_categories:\n",
    "    subset = yearly_bias[yearly_bias['bias_category'] == category].sort_values('year')\n",
    "    signal = subset['incident_count'].values\n",
    "    years = subset['year'].values\n",
    "\n",
    "    if len(signal) < 6:\n",
    "        continue  # skip too-short series\n",
    "\n",
    "    # Apply PELT change point detection\n",
    "    model = rpt.Pelt(model=\"l2\").fit(signal)\n",
    "    change_points = model.predict(pen=10)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(years, signal, label=f'{category} Incidents')\n",
    "    for cp in change_points[:-1]:\n",
    "        plt.axvline(x=years[cp], color='red', linestyle='--', alpha=0.7)\n",
    "    plt.title(f'Change Point Detection - {category}')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Incident Count')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Print change years\n",
    "    change_years = years[change_points[:-1]]\n",
    "    print(f'{category} Change Years:', change_years.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400ada3-bf1f-4371-87f3-26b84ce1d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df = model_data.copy()\n",
    "last_known_year = future_df['year'].max()\n",
    "future_years = [2024, 2025, 2026]\n",
    "\n",
    "# Columns used for prediction\n",
    "feature_cols = list(X.columns)\n",
    "\n",
    "# Storage for future predictions\n",
    "future_preds = []\n",
    "\n",
    "# Recursive Forecast Loop\n",
    "for future_year in future_years:\n",
    "    new_rows = []\n",
    "\n",
    "    for bias_cat in future_df['bias_category'].unique():\n",
    "        # Get last 5 rows for this bias category\n",
    "        group = future_df[future_df['bias_category'] == bias_cat].sort_values('year').tail(5).copy()\n",
    "\n",
    "        if group.shape[0] < 2:\n",
    "            continue  # skip if not enough history\n",
    "\n",
    "        # Compute new features\n",
    "        incident_lag1 = group.iloc[-1]['incident_count']\n",
    "        incident_lag2 = group.iloc[-2]['incident_count']\n",
    "        rolling_avg = group['incident_count'].rolling(3, min_periods=1).mean().iloc[-1]\n",
    "        pct_change = (group.iloc[-1]['incident_count'] - group.iloc[-2]['incident_count']) / (group.iloc[-2]['incident_count'] + 1e-6)\n",
    "\n",
    "        # Construct input row\n",
    "        input_row = {\n",
    "            'year': future_year,\n",
    "            'incident_rolling': rolling_avg,\n",
    "            'yoy_change': pct_change,\n",
    "            'incident_lag1': incident_lag1,\n",
    "            'incident_lag2': incident_lag2\n",
    "        }\n",
    "\n",
    "        # Add encoded bias_category columns (same as training one-hot)\n",
    "        for col in encoder.get_feature_names_out(['bias_category']):\n",
    "            input_row[col] = 1 if col == f\"bias_category_{bias_cat}\" else 0\n",
    "\n",
    "        # Ensure all expected features are present\n",
    "        input_df = pd.DataFrame([input_row], columns=feature_cols)\n",
    "\n",
    "        # Predict with LightGBM\n",
    "        pred = best_lgb.predict(input_df)[0]\n",
    "        input_row['incident_count'] = pred\n",
    "        input_row['bias_category'] = bias_cat\n",
    "\n",
    "        new_rows.append(input_row)\n",
    "\n",
    "    # Add predictions to future_df so they can feed next year's forecast\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    future_df = pd.concat([future_df, new_df], ignore_index=True)\n",
    "    future_preds.append(new_df)\n",
    "\n",
    "# Combine all forecasts into one DataFrame\n",
    "future_forecast_df = pd.concat(future_preds, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30e8b2-8254-4aa1-88aa-bed76d0f72d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=future_df[future_df['year'] >= 2018], x='year', y='incident_count', hue='bias_category')\n",
    "plt.title(\"Forecasted Hate Crime Incidents (2024â€“2026)\")\n",
    "plt.ylabel(\"Predicted Incidents\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b1989f-16a1-4f97-8027-8d66c24c62b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe7db9-8e2e-43d2-bee3-191d0a9a264e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
